{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "hpKHK1VhwA4q",
        "outputId": "c070cd71-f3d9-48bf-d18a-b6c73a690156"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/home/admin/anaconda3/lib/python3.9/site-packages/pyspark'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import the libary\n",
        "import findspark\n",
        "# Initiate findspark\n",
        "findspark.init()\n",
        "# Check the location for Spark\n",
        "findspark.find()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "Gt8Ahhtw1hTU",
        "outputId": "27de6924-460c-45e2-9f97-5ee2bd3a93fe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import databricks.koalas as ks\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import LongType, DoubleType\n",
        "from pyspark.sql.functions import col,isnan,when,count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22/12/07 16:12:48 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
          ]
        }
      ],
      "source": [
        "spark = SparkSession.builder.master(\"local[1]\") \\\n",
        "                    .appName('SparkByExamples.com') \\\n",
        "                    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "meta=spark.read\\\n",
        ".format(\"com.databricks.spark.csv\")\\\n",
        ".option(\"multiline\",True)\\\n",
        ".option(\"header\",True)\\\n",
        ".option(\"escape\", \"\\\"\")\\\n",
        ".option(\"inferschema\",True)\\\n",
        ".csv(\"data/movies_metadata.csv\")\n",
        "\n",
        "keywords=spark.read\\\n",
        ".format(\"com.databricks.spark.csv\")\\\n",
        ".option(\"multiline\",True)\\\n",
        ".option(\"header\",True)\\\n",
        ".option(\"escape\", \"\\\"\")\\\n",
        ".option(\"inferschema\",True)\\\n",
        ".csv('data/keywords.csv')\n",
        "\n",
        "links=spark.read\\\n",
        ".format(\"com.databricks.spark.csv\")\\\n",
        ".option(\"multiline\",True)\\\n",
        ".option(\"header\",True)\\\n",
        ".option(\"escape\", \"\\\"\")\\\n",
        ".option(\"inferschema\",True)\\\n",
        ".csv('data/links.csv')\n",
        "\n",
        "ratings=spark.read\\\n",
        ".format(\"com.databricks.spark.csv\")\\\n",
        ".option(\"multiline\",True)\\\n",
        ".option(\"header\",True)\\\n",
        ".option(\"escape\", \"\\\"\")\\\n",
        ".option(\"inferschema\",True)\\\n",
        ".csv('data/ratings.csv')\n",
        "\n",
        "credits=spark.read\\\n",
        ".format(\"com.databricks.spark.csv\")\\\n",
        ".option(\"multiline\",True)\\\n",
        ".option(\"header\",True)\\\n",
        ".option(\"escape\", \"\\\"\")\\\n",
        ".option(\"inferschema\",True)\\\n",
        ".csv('data/credits.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- adult: string (nullable = true)\n",
            " |-- belongs_to_collection: string (nullable = true)\n",
            " |-- budget: string (nullable = true)\n",
            " |-- genres: string (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- original_language: string (nullable = true)\n",
            " |-- overview: string (nullable = true)\n",
            " |-- popularity: double (nullable = true)\n",
            " |-- production_companies: string (nullable = true)\n",
            " |-- production_countries: string (nullable = true)\n",
            " |-- release_date: string (nullable = true)\n",
            " |-- revenue: long (nullable = true)\n",
            " |-- runtime: double (nullable = true)\n",
            " |-- tagline: string (nullable = true)\n",
            " |-- vote_average: double (nullable = true)\n",
            " |-- vote_count: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "meta = meta.withColumn(\"popularity\",meta.popularity.cast(DoubleType()))\n",
        "meta = meta.drop('homepage','imdb_id','original_title','poster_path','spoken_languages','status','title','video')\n",
        "meta.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-RECORD 0----------------------\n",
            " adult                 | 0     \n",
            " belongs_to_collection | 42226 \n",
            " budget                | 0     \n",
            " genres                | 0     \n",
            " id                    | 0     \n",
            " original_language     | 11    \n",
            " overview              | 973   \n",
            " popularity            | 6     \n",
            " production_companies  | 4     \n",
            " production_countries  | 3     \n",
            " release_date          | 87    \n",
            " revenue               | 6     \n",
            " runtime               | 263   \n",
            " tagline               | 25058 \n",
            " vote_average          | 6     \n",
            " vote_count            | 6     \n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "meta.select([count(when(col(c).contains('None') | \\\n",
        "                            col(c).contains('NULL') | \\\n",
        "                            (col(c) == '' ) | \\\n",
        "                            col(c).isNull() | \\\n",
        "                            isnan(c), c \n",
        "                           )).alias(c)\n",
        "                    for c in meta.columns]).show(vertical=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "meta = meta.filter((meta.vote_average.isNotNull()) & (meta.release_date.isNotNull()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#credits = credits.withColumnRenamed('id', 'credits_id')\n",
        "\n",
        "#meta.join(credits,meta.id ==  credits.credits_id,\"left\").drop('credits_id') \\\n",
        "#     .show(vertical=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.regression import DecisionTreeRegressor\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "# predict revenue et vote_average\n",
        "(train_data, test_data) = meta.randomSplit([0.7, 0.3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "ename": "IllegalArgumentException",
          "evalue": "requirement failed: Column vote_average must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.DoubleType$:double.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[80], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m featureIndexer \u001b[39m=\u001b[39m\\\n\u001b[0;32m----> 2\u001b[0m     VectorIndexer(inputCol\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mvote_average\u001b[39;49m\u001b[39m\"\u001b[39;49m, outputCol\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mindexed_vote_average\u001b[39;49m\u001b[39m\"\u001b[39;49m, maxCategories\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m)\u001b[39m.\u001b[39;49mfit(meta)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyspark/ml/wrapper.py:383\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset: DataFrame) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m JM:\n\u001b[0;32m--> 383\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_java(dataset)\n\u001b[1;32m    384\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    385\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copyValues(model)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyspark/ml/wrapper.py:380\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 380\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mfit(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Column vote_average must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.DoubleType$:double."
          ]
        }
      ],
      "source": [
        "featureassembler = VectorAssembler(inputCols = [\"A MODFIER\"], outputCol = \"A MODIFIER\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output = featureassembler.transform(meta)\n",
        "\n",
        "output.select(\"A CHANGER\").show()\n",
        "finalised_data = output.select(\"A CHANGER\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "regressor = LinearRegression(featuresCol = 'Independent Features', labelCol = 'Yearly Amount Spent')\n",
        "regressor = regressor.fit(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_results = regressor.evaluate(test_data)\n",
        "pred_results.predictions.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (1756570603.py, line 17)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[79], line 17\u001b[0;36m\u001b[0m\n\u001b[0;31m    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "dt = DecisionTreeRegressor(featuresCol=\"indexedFeatures\")\n",
        "\n",
        "# Chain indexer and tree in a Pipeline\n",
        "pipeline = Pipeline(stages=[featureIndexer, dt])\n",
        "\n",
        "# Train model.  This also runs the indexer.\n",
        "model = pipeline.fit(trainingData)\n",
        "\n",
        "# Make predictions.\n",
        "predictions = model.transform(testData)\n",
        "\n",
        "# Select example rows to display.\n",
        "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
        "\n",
        "# Select (prediction, true label) and compute test error\n",
        "evaluator = RegressionEvaluator(Âµ\n",
        "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
        "\n",
        "treeModel = model.stages[1]\n",
        "# summary only\n",
        "print(treeModel)\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "09677b35d0094de7b6ccbefc0f0b0a8fee9396b45d22da837d7e63f3e987ce80"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
